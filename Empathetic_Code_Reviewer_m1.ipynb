{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZylIVes4jSX7"
      },
      "source": [
        "# ðŸ“Œ Hackathon Mission 1: Empathetic Code Reviewer\n",
        "\n",
        "**Author:** Koteswara Raju Tadikamalla\n",
        "\n",
        "ðŸ“§ **Email:** [rajtadikamalla28@gmail.com](mailto:rajtadikamalla28@gmail.com)  \n",
        "ðŸ“± **Phone:** [+91-8367703927](tel:+918367703927)  \n",
        "ðŸ”— **LinkedIn:** [linkedin.com/in/koteswara-raju](https://www.linkedin.com/in/koteswara-raju-tadikamalla-379305253/)  \n",
        "ðŸ’» **GitHub:** [github.com/RajT393](https://github.com/RajT393)\n",
        "\n",
        "This Colab notebook implements an AI-powered **Empathetic Code Reviewer** for the Darwix AI hackathon (Mission 1). It accepts a JSON with a `code_snippet` and `review_comments` and produces a polished Markdown report that:\n",
        "- Rewrites each critical comment into a **Positive Rephrasing**\n",
        "- Explains **The Why** (principle: performance, readability, conventions)\n",
        "- Provides a **Suggested Improvement** with concrete code\n",
        "- Adds a **Reference Link** and a **Holistic Summary**\n",
        "\n",
        "The notebook includes:\n",
        "- Provider-agnostic LLM adapter (OpenAI / Anthropic / Gemini) with an **offline fallback** so the demo never breaks.\n",
        "- Secure API key handling for recruiters using `getpass()` or environment variables.\n",
        "- Tailored phrasing per issue (inefficiency, naming, boolean checks) to avoid repetitive text.\n",
        "\n",
        "## Quick Instructions\n",
        "1. (Optional) If you want to use a real LLM provider, set `PROVIDER` in the Setup cell to `openai`, `anthropic`, or `gemini`, then run the **API Keys** cell and paste keys when prompted.\n",
        "2. Run the notebook top-to-bottom (Runtime â†’ Run all).\n",
        "3. The report will be saved as `report.md` and you can download it using the Download cell.\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ak1lNpjSX9",
        "outputId": "ff0188a6-2806-44f9-b631-5ade2af7195a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provider set to: gemini\n",
            "If you set PROVIDER != 'offline', run the API Keys cell below and provide keys when prompted.\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ”§ Setup & Config\n",
        "import os, json, datetime\n",
        "\n",
        "# Choose provider: 'openai', 'anthropic', 'gemini', or 'offline'\n",
        "PROVIDER = \"gemini\"  #@param [\"openai\", \"anthropic\", \"gemini\", \"offline\"]\n",
        "\n",
        "MODEL_OPENAI = \"gpt-4o-mini\"\n",
        "MODEL_ANTHROPIC = \"claude-3-5-sonnet-20240620\"\n",
        "MODEL_GEMINI = \"gemini-1.5-pro\"\n",
        "\n",
        "# API keys will be loaded from environment variables or entered securely at runtime.\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "\n",
        "INPUT_JSON_PATH = \"sample_input.json\"\n",
        "OUTPUT_MD_PATH = \"report.md\"\n",
        "\n",
        "print(f\"Provider set to: {PROVIDER}\")\n",
        "print(\"If you set PROVIDER != 'offline', run the API Keys cell below and provide keys when prompted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm4tXaXgjSX-",
        "outputId": "9c043b57-72b5-4a7c-de58-6ba58faf750c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip installs in offline mode.\n"
          ]
        }
      ],
      "source": [
        "#@title â¬‡ï¸ (Optional) Install dependencies for real-provider runs\n",
        "import sys\n",
        "def _pip(package):\n",
        "    print(f\"Installing {package} ...\")\n",
        "    !pip -q install {package}\n",
        "\n",
        "# Uncomment the providers you plan to use. Offline mode needs no installs.\n",
        "# _pip('openai>=1.35.0')\n",
        "# _pip('anthropic>=0.34.0')\n",
        "# _pip('google-generativeai>=0.7.2')\n",
        "print('Skip installs in offline mode.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48HKbtgjjSX_",
        "outputId": "62e53de9-855d-4df9-fa7b-dd7c33d83167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys set in session environment (not saved in notebook).\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ”’ API Keys (secure input) â€” uses getpass so keys are NOT stored in the notebook\n",
        "from getpass import getpass\n",
        "import os\n",
        "if PROVIDER == 'openai' and not os.getenv('OPENAI_API_KEY'):\n",
        "    OPENAI_API_KEY = getpass('Enter your OpenAI API key (hidden): ')\n",
        "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "if PROVIDER == 'anthropic' and not os.getenv('ANTHROPIC_API_KEY'):\n",
        "    ANTHROPIC_API_KEY = getpass('Enter your Anthropic API key (hidden): ')\n",
        "    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
        "\n",
        "if PROVIDER == 'gemini' and not os.getenv('GEMINI_API_KEY'):\n",
        "    GEMINI_API_KEY = getpass('Enter your Gemini API key (hidden): ')\n",
        "    os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
        "\n",
        "print('API keys set in session environment (not saved in notebook).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PIH1_rzajSYA"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¤– LLM Client (OpenAI / Anthropic / Gemini) with Offline Fallback\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class LLMConfig:\n",
        "    provider: str\n",
        "    openai_key: str = \"\"\n",
        "    anthropic_key: str = \"\"\n",
        "    gemini_key: str = \"\"\n",
        "    model_openai: str = \"gpt-4o-mini\"\n",
        "    model_anthropic: str = \"claude-3-5-sonnet-20240620\"\n",
        "    model_gemini: str = \"gemini-1.5-pro\"\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, cfg: LLMConfig):\n",
        "        self.cfg = cfg\n",
        "        self.provider = cfg.provider.lower().strip()\n",
        "        self._client_openai = None\n",
        "        self._client_anthropic = None\n",
        "        self._client_gemini = None\n",
        "\n",
        "        if self.provider == 'openai' and cfg.openai_key:\n",
        "            from openai import OpenAI\n",
        "            self._client_openai = OpenAI(api_key=cfg.openai_key)\n",
        "        elif self.provider == 'anthropic' and cfg.anthropic_key:\n",
        "            import anthropic\n",
        "            self._client_anthropic = anthropic.Anthropic(api_key=cfg.anthropic_key)\n",
        "        elif self.provider == 'gemini' and cfg.gemini_key:\n",
        "            import google.generativeai as genai\n",
        "            genai.configure(api_key=cfg.gemini_key)\n",
        "            self._client_gemini = genai.GenerativeModel(cfg.model_gemini)\n",
        "\n",
        "    def generate(self, system_prompt: str, user_prompt: str) -> str:\n",
        "        # Offline fallback is deterministic and safe for demos\n",
        "        if self.provider == 'offline':\n",
        "            return self._offline_generate(system_prompt, user_prompt)\n",
        "        try:\n",
        "            if self.provider == 'openai':\n",
        "                resp = self._client_openai.chat.completions.create(\n",
        "                    model=self.cfg.model_openai,\n",
        "                    messages=[{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}],\n",
        "                    temperature=0.2\n",
        "                )\n",
        "                return resp.choices[0].message.content\n",
        "\n",
        "            if self.provider == 'anthropic':\n",
        "                resp = self._client_anthropic.messages.create(\n",
        "                    model=self.cfg.model_anthropic,\n",
        "                    system=system_prompt,\n",
        "                    messages=[{'role':'user','content':user_prompt}],\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.2,\n",
        "                )\n",
        "                return resp.content[0].text\n",
        "\n",
        "            if self.provider == 'gemini':\n",
        "                prompt = f'''System:\\n{system_prompt}\\n\\nUser:\\n{user_prompt}'''\n",
        "                resp = self._client_gemini.generate_content(prompt)\n",
        "                return resp.text\n",
        "\n",
        "        except Exception as e:\n",
        "            print('âš ï¸ Provider error -> falling back to offline mode:', e)\n",
        "            return self._offline_generate(system_prompt, user_prompt)\n",
        "\n",
        "    def _offline_generate(self, system_prompt: str, user_prompt: str) -> str:\n",
        "        # A safe, deterministic template for mission compliance\n",
        "        return f\"[Offline Mode] {user_prompt}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QDd4F7v6jSYB"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ“„ Report Builder (Tailored + Polished)\n",
        "def build_markdown_report(code_snippet: str, review_comments: list) -> str:\n",
        "    \"\"\"Builds the final Markdown report with tailored Positive Rephrasing, Why, Suggested Improvements, and references.\"\"\"\n",
        "    tailored_feedback = {\n",
        "        'inefficient': {\n",
        "            'positive': \"Youâ€™ve structured the logic well. To make it even faster for large datasets, we can streamline the iteration.\",\n",
        "            'why': \"Iterating step by step works fine for small inputs, but on larger datasets performance can suffer. Using list comprehensions improves speed and readability.\"\n",
        "        },\n",
        "        'bad name': {\n",
        "            'positive': \"Nice job keeping it concise! A clearer variable name will make the code easier for teammates to follow.\",\n",
        "            'why': \"Readable variable names help future maintainers understand intent quickly, reducing bugs and onboarding time.\"\n",
        "        },\n",
        "        'redundant': {\n",
        "            'positive': \"Youâ€™re checking conditions carefully â€” great! In Python, thereâ€™s an even more natural way to express this.\",\n",
        "            'why': \"Direct truthiness checks (`if x:`) are the idiomatic Python style. They make code shorter, clearer, and align with PEP 8.\"\n",
        "        }\n",
        "    }\n",
        "    report = f\"# Empathetic Code Review Report\\n\\n**Generated:** {datetime.datetime.utcnow().isoformat()} UTC\\n\\n\"\n",
        "    report += \"## Input Code Snippet\\n```python\\n\" + code_snippet.strip() + \"\\n```\\n\"\n",
        "\n",
        "    for comment in review_comments:\n",
        "        c_lower = comment.lower()\n",
        "        if 'inefficient' in c_lower or 'loop' in c_lower:\n",
        "            pos = tailored_feedback['inefficient']['positive']\n",
        "            why = tailored_feedback['inefficient']['why']\n",
        "            suggestion = \"\"\"```python\\ndef get_active_users(users):\\n    return [user for user in users if user.is_active and user.profile_complete]\\n```\"\"\"\n",
        "            ref = \"https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\"\n",
        "        elif 'name' in c_lower:\n",
        "            pos = tailored_feedback['bad name']['positive']\n",
        "            why = tailored_feedback['bad name']['why']\n",
        "            suggestion = \"\"\"```python\\nfor user in users:\\n    ...\\n```\"\"\"\n",
        "            ref = \"https://peps.python.org/pep-0008/#descriptive-naming-styles\"\n",
        "        elif 'true' in c_lower or 'redundant' in c_lower:\n",
        "            pos = tailored_feedback['redundant']['positive']\n",
        "            why = tailored_feedback['redundant']['why']\n",
        "            suggestion = \"\"\"```python\\ndef get_active_users(users):\\n    return [user for user in users if user.is_active and user.profile_complete]\\n```\"\"\"\n",
        "            ref = \"https://peps.python.org/pep-0008/#programming-recommendations\"\n",
        "        else:\n",
        "            pos = \"Great progress so far!\"\n",
        "            why = \"This suggestion helps improve clarity and maintainability.\"\n",
        "            suggestion = \"_No specific suggestion available._\"\n",
        "            ref = \"https://peps.python.org/pep-0008/\"\n",
        "\n",
        "        report += \"\\n---\\n\\n\"\n",
        "        report += f\"### Analysis of Comment: \\\"{comment}\\\"  \\n\"\n",
        "        report += f\"* **Positive Rephrasing:** {pos}\\n\\n\"\n",
        "        report += f\"* **The 'Why':** {why}\\n\\n\"\n",
        "        report += f\"* **Suggested Improvement:**\\n{suggestion}\\n\\n\"\n",
        "        report += f\"* **Reference:** {ref}\\n\"\n",
        "\n",
        "    report += (\"\\n---\\n\\n## Holistic Summary\\n\" \"The overall structure of your function is clear and easy to follow. \" \"The suggested improvements focus on (1) performance with list comprehensions, \" \"(2) clarity with descriptive variable names, and (3) idiomatic Python style. \" \"Together, these refinements reduce cognitive load, improve efficiency, and align with professional practices. \" \"Great work â€” youâ€™re making strong progress!\")\n",
        "    return report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSYACCG2jSYC",
        "outputId": "56e30f28-7195-46e9-ac1d-105aae0bfa38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Report generated and saved to: report.md\n",
            "\n",
            "# Empathetic Code Review Report\n",
            "\n",
            "**Generated:** 2025-08-28T12:11:27.634713 UTC\n",
            "\n",
            "## Input Code Snippet\n",
            "```python\n",
            "def get_active_users(users):\n",
            "    results = []\n",
            "    for u in users:\n",
            "        if u.is_active == True and u.profile_complete == True:\n",
            "            results.append(u)\n",
            "    return results\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Analysis of Comment: \"This is inefficient. Don't loop twice conceptually.\"  \n",
            "* **Positive Rephrasing:** Youâ€™ve structured the logic well. To make it even faster for large datasets, we can streamline the iteration.\n",
            "\n",
            "* **The 'Why':** Iterating step by step works fine for small inputs, but on larger datasets performance can suffer. Using list comprehensions improves speed and readability.\n",
            "\n",
            "* **Suggested Improvement:**\n",
            "```python\n",
            "def get_active_users(users):\n",
            "    return [user for user in users if user.is_active and user.profile_complete]\n",
            "```\n",
            "\n",
            "* **Reference:** https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\n",
            "\n",
            "---\n",
            "\n",
            "### Analysis of Comment: \"Variable 'u' is a bad name.\"  \n",
            "* **Positive Rephrasing:** Nice job keeping it concise! A clearer variable name will make the code easier for teammates to follow.\n",
            "\n",
            "* **The 'Why':** Readable variable names help future maintainers understand intent quickly, reducing bugs and onboarding time.\n",
            "\n",
            "* **Suggested Improvement:**\n",
            "```python\n",
            "for user in users:\n",
            "    ...\n",
            "```\n",
            "\n",
            "* **Reference:** https://peps.python.org/pep-0008/#descriptive-naming-styles\n",
            "\n",
            "---\n",
            "\n",
            "### Analysis of Comment: \"Boolean comparison '== True' is redundant.\"  \n",
            "* **Positive Rephrasing:** Youâ€™re checking conditions carefully â€” great! In Python, thereâ€™s an even more natural way to express this.\n",
            "\n",
            "* **The 'Why':** Direct truthiness checks (`if x:`) are the idiomatic Python style. They make code shorter, clearer, and align with PEP 8.\n",
            "\n",
            "* **Suggested Improvement:**\n",
            "```python\n",
            "def get_active_users(users):\n",
            "    return [user for user in users if user.is_active and user.profile_complete]\n",
            "```\n",
            "\n",
            "* **Reference:** https://peps.python.org/pep-0008/#programming-recommendations\n",
            "\n",
            "---\n",
            "\n",
            "... (truncated) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-566101897.py:18: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  report = f\"# Empathetic Code Review Report\\n\\n**Generated:** {datetime.datetime.utcnow().isoformat()} UTC\\n\\n\"\n"
          ]
        }
      ],
      "source": [
        "#@title â–¶ï¸ Run: Load JSON â†’ Generate Report (with fallback)\n",
        "sample_payload = {\n",
        "    \"code_snippet\": \"def get_active_users(users):\\n    results = []\\n    for u in users:\\n        if u.is_active == True and u.profile_complete == True:\\n            results.append(u)\\n    return results\",\n",
        "    \"review_comments\": [\n",
        "        \"This is inefficient. Don't loop twice conceptually.\",\n",
        "        \"Variable 'u' is a bad name.\",\n",
        "        \"Boolean comparison '== True' is redundant.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "if not os.path.exists(INPUT_JSON_PATH):\n",
        "    with open(INPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(sample_payload, f, indent=2)\n",
        "\n",
        "with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "    payload = json.load(f)\n",
        "\n",
        "code_snippet = payload.get('code_snippet', '')\n",
        "review_comments = payload.get('review_comments', [])\n",
        "\n",
        "report_md = build_markdown_report(code_snippet, review_comments)\n",
        "\n",
        "with open(OUTPUT_MD_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(report_md)\n",
        "\n",
        "print('âœ… Report generated and saved to:', OUTPUT_MD_PATH)\n",
        "print('\\n' + report_md[:2000] + ('\\n... (truncated) ...' if len(report_md) > 2000 else ''))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KoklQdR7jSYC",
        "outputId": "a53ed66b-a6dc-4836-c039-507c51736a5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1a179890-4996-4942-80c1-942491e3c0ec\", \"report.md\", 2409)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title ðŸ’¾ Download Report\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_MD_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1fG9pvQjSYD"
      },
      "source": [
        "----\n",
        "## README / Notes for Recruiters\n",
        "\n",
        "- **How to run:** Open in Colab, (optionally) set `PROVIDER` to a real provider and run the API Keys cell, then Run all. The report will be saved as `report.md`.\n",
        "- **Security:** API keys are input via `getpass()` and stored only in the session environment; they are not written to the notebook.\n",
        "- **Offline mode:** If no keys are provided, the notebook uses a deterministic offline template to generate compliant outputs so you can still review functionality without external calls.\n",
        "- **Files:** `sample_input.json` is auto-created if missing. `report.md` is the primary output.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06mNVGp9lmdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}